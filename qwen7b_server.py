# -*- coding: utf-8 -*-
"""
Qwen LLMæœåŠ¡ç«¯ + Webå¯è§†åŒ–ç•Œé¢

åŠŸèƒ½ï¼š
1. æä¾›Flask APIæœåŠ¡ï¼Œå°è£…Qwen-1.8B-Chatæ¨¡å‹
2. æä¾›Webå¯è§†åŒ–ç•Œé¢ï¼Œç”¨æˆ·å¯åœ¨æµè§ˆå™¨ä¸­ç›´æ¥ä½¿ç”¨
3. å¯é€‰ï¼šé›†æˆRAGåŠŸèƒ½ï¼Œæ”¯æŒçŸ¥è¯†å›¾è°±æ£€ç´¢å¢å¼ºç”Ÿæˆ

ä¸»è¦æ¥å£ï¼š
- GET  / : Webå¯è§†åŒ–ç•Œé¢
- POST /generate : åŸºç¡€LLMç”Ÿæˆæ¥å£
- POST /rag : RAGå®Œæ•´é—®ç­”æ¥å£ï¼ˆå¯é€‰ï¼‰

ä½œè€…ï¼šåŒ»ç–—çŸ¥è¯†å›¾è°±é—®ç­”ç³»ç»Ÿâ€”â€”ä½•é˜³
æ—¥æœŸï¼š2025-12-20
"""

import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation.utils import GenerationConfig
import json
from flask import Flask, request, jsonify

# å¯¼å…¥RAGç›¸å…³æ¨¡å—ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸éœ€è¦RAGåŠŸèƒ½å¯ä»¥æ³¨é‡Šæ‰ï¼‰
try:
    from question_classifier import QuestionClassifier
    from build_medicalgraph import MedicalGraph
    from llm_server import ModelAPI
    RAG_AVAILABLE = True
except ImportError:
    RAG_AVAILABLE = False
    print("[WARNING] RAGæ¨¡å—æœªå¯¼å…¥ï¼Œå®Œæ•´é—®ç­”åŠŸèƒ½ä¸å¯ç”¨")


## æ³¨æ„
# è¿™é‡Œæ”¹ä¸ºä½¿ç”¨ HuggingFace ä¸Šçš„å°æ¨¡å‹ Qwen-1.8B-Chatï¼ˆè‡ªåŠ¨ä¸‹è½½åˆ°æœ¬åœ°ç¼“å­˜ï¼‰
MODEL_NAME = "Qwen/Qwen-1_8B-Chat"

# è®¾å¤‡é…ç½®ï¼šä¼˜å…ˆä½¿ç”¨GPUï¼Œæ²¡æœ‰GPUæ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°CPU
use_gpu = torch.cuda.is_available()
device = torch.device("cuda") if use_gpu else torch.device("cpu")

# æ•°æ®ç±»å‹é…ç½®ï¼šGPUä½¿ç”¨fp16ï¼ˆåŠ é€Ÿå¹¶èŠ‚çœæ˜¾å­˜ï¼‰ï¼ŒCPUä½¿ç”¨fp32ï¼ˆä¿è¯ç²¾åº¦ï¼‰
dtype = torch.float16 if use_gpu else torch.float32

# åŠ è½½åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

# ==================== æ¨¡å‹åŠ è½½ ====================
if use_gpu:
    # GPUæ¨¡å¼ï¼šç›´æ¥åŠ è½½åˆ° cuda:0ï¼Œä½¿ç”¨fp16ç²¾åº¦
    print(f"[INFO] ä½¿ç”¨GPUæ¨¡å¼ï¼Œè®¾å¤‡: {device}")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        trust_remote_code=True,
        torch_dtype=dtype,
        device_map={"": 0},  # å•å¡GPUï¼Œæ˜ å°„åˆ°è®¾å¤‡0
    )
else:
    # CPUæ¨¡å¼ï¼šå¼€å¯ low_cpu_mem_usage ä»¥é™ä½å³°å€¼å†…å­˜å ç”¨
    print(f"[INFO] ä½¿ç”¨CPUæ¨¡å¼")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        trust_remote_code=True,
        torch_dtype=dtype,
        device_map={"": "cpu"},
        low_cpu_mem_usage=True,  # é™ä½å†…å­˜å³°å€¼
    ).to(device)

# åŠ è½½ç”Ÿæˆé…ç½®
model.generation_config = GenerationConfig.from_pretrained(MODEL_NAME)
print("[INFO] æ¨¡å‹åŠ è½½å®Œæˆ")

# ==================== æ¨¡å‹æ¨ç†å‡½æ•° ====================
def predict_model(data):
    """
    ä½¿ç”¨Qwenæ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆ
    
    å‚æ•°:
        data (dict): åŒ…å«ä»¥ä¸‹å­—æ®µçš„å­—å…¸
            - message: [{"content": "ç”¨æˆ·è¾“å…¥çš„é—®é¢˜"}]
            - max_tokens: æœ€å¤§ç”Ÿæˆé•¿åº¦ï¼ˆå¯é€‰ï¼Œé»˜è®¤64ï¼Œæœ€å¤§256ï¼‰
    
    è¿”å›:
        str: æ¨¡å‹ç”Ÿæˆçš„å›ç­”æ–‡æœ¬
    
    æ³¨æ„:
        - ä½¿ç”¨ model.chat æ¥å£ï¼ˆå®˜æ–¹æ¨èï¼‰ï¼Œè‡ªåŠ¨å¤„ç†promptæ¨¡æ¿
        - è‡ªåŠ¨å¤„ç†è¾“å…¥é•¿åº¦é™åˆ¶å’Œæ˜¾å­˜æº¢å‡º
        - æ”¯æŒGPU/CPUè‡ªåŠ¨åˆ‡æ¢
    """
    text = data["message"][0]["content"]
    
    # é™åˆ¶è¾“å…¥é•¿åº¦ï¼Œé¿å…æ˜¾å­˜æº¢å‡ºï¼ˆ6GB GPU å»ºè®®ä¸è¶…è¿‡ 2000 tokensï¼‰
    max_input_tokens = 2000
    tokens = tokenizer.encode(text, add_special_tokens=False)
    if len(tokens) > max_input_tokens:
        # æˆªæ–­åˆ°æœ€å¤§é•¿åº¦ï¼Œä¿ç•™å‰é¢çš„å†…å®¹
        truncated_tokens = tokens[:max_input_tokens]
        text = tokenizer.decode(truncated_tokens, skip_special_tokens=True)
        print(f"[WARNING] è¾“å…¥è¿‡é•¿ï¼Œå·²æˆªæ–­åˆ° {max_input_tokens} tokens")

    # é»˜è®¤æœ€å¤§ç”Ÿæˆé•¿åº¦é€‚ä¸­ï¼Œå…¼é¡¾æ—¶å»¶å’Œå®Œæ•´æ€§
    max_new_tokens = data.get("max_tokens", 64)
    # é™åˆ¶æœ€å¤§ç”Ÿæˆé•¿åº¦ï¼Œé¿å…æ˜¾å­˜æº¢å‡º
    max_new_tokens = min(max_new_tokens, 256)

    # Qwen chat æ¥å£æœ¬èº«ä¸ç›´æ¥æ”¯æŒ max_new_tokens å½¢å‚ï¼Œ
    # é€šè¿‡ä¸´æ—¶ä¿®æ”¹ generation_config æ¥æ§åˆ¶
    old_max_new_tokens = model.generation_config.max_new_tokens
    model.generation_config.max_new_tokens = max_new_tokens
    
    # æ¸…ç†æ˜¾å­˜ç¼“å­˜ï¼ˆå¦‚æœä½¿ç”¨ GPUï¼‰
    if use_gpu:
        torch.cuda.empty_cache()
    
    try:
        response, _ = model.chat(tokenizer, query=text, history=[])
    except TypeError:
        response, _ = model.chat(tokenizer, text, history=[])
    except torch.cuda.OutOfMemoryError as oom_err:
        # æ˜¾å­˜ä¸è¶³æ—¶æ¸…ç†ç¼“å­˜å¹¶é‡è¯•ä¸€æ¬¡
        if use_gpu:
            torch.cuda.empty_cache()
            print("[WARNING] CUDA OOMï¼Œå·²æ¸…ç†ç¼“å­˜ï¼Œå°è¯•ç¼©çŸ­è¾“å…¥é‡è¯•...")
            # è¿›ä¸€æ­¥ç¼©çŸ­è¾“å…¥
            shorter_tokens = tokens[:1000] if len(tokens) > 1000 else tokens
            text = tokenizer.decode(shorter_tokens, skip_special_tokens=True)
            model.generation_config.max_new_tokens = 32
            try:
                response, _ = model.chat(tokenizer, query=text, history=[])
            except:
                raise Exception("æ˜¾å­˜ä¸è¶³ï¼Œå³ä½¿ç¼©çŸ­è¾“å…¥åä»æ— æ³•å¤„ç†ã€‚è¯·å‡å°‘è¾“å…¥é•¿åº¦æˆ–é‡å¯æœåŠ¡é‡Šæ”¾æ˜¾å­˜ã€‚")
        else:
            raise oom_err
    finally:
        # è¿˜åŸåŸå§‹é…ç½®ï¼Œé¿å…å½±å“åç»­è°ƒç”¨
        model.generation_config.max_new_tokens = old_max_new_tokens
        # å†æ¬¡æ¸…ç†æ˜¾å­˜
        if use_gpu:
            torch.cuda.empty_cache()

    return response

# ==================== Flaskåº”ç”¨åˆå§‹åŒ– ====================
app = Flask(import_name=__name__)

# ==================== APIè·¯ç”± ====================
@app.route("/generate", methods=["POST", "GET"])
def generate():
    """
    åŸºç¡€LLMç”Ÿæˆæ¥å£
    
    æ”¯æŒçš„è°ƒç”¨æ–¹å¼ï¼š
    1. POST JSONæ ¼å¼ï¼š
       {
         "message": [{"content": "ç”¨æˆ·é—®é¢˜"}],
         "max_tokens": 256  // å¯é€‰ï¼Œé»˜è®¤64ï¼Œæœ€å¤§256
       }
    
    2. GET æŸ¥è¯¢å‚æ•°ï¼š
       /generate?q=ç”¨æˆ·é—®é¢˜
    
    è¿”å›æ ¼å¼ï¼š
    {
        "output": ["ç”Ÿæˆçš„ç­”æ¡ˆ"],
        "status": "success" | "error",
        "history": []
    }
    """
    try:
        # ä¼˜å…ˆå°è¯•è§£æ JSON ä½“
        data = request.get_json(silent=True)
        if not data:
            # å…¼å®¹ GET å‚æ•°ï¼Œæˆ–è¡¨å•/ç©º body çš„ POST
            q = request.args.get("q", "").strip()
            if not q:
                return jsonify({"output": [""], "status": "error", "history": [], "msg": "ç¼ºå°‘è¾“å…¥å†…å®¹"}), 400
            data = {"message": [{"content": q}], "max_tokens": 64}
    except Exception as parse_err:
        return jsonify({"output": [""], "status": "error", "history": [], "msg": f"è¯·æ±‚è§£æå¤±è´¥: {parse_err}"}), 400

    print("request payload:", data)

    try:
        res = predict_model(data)
        label = "success"
    except Exception as e:
        import traceback
        traceback.print_exc()  # æ‰“å°å®Œæ•´å †æ ˆä¾¿äºæ’æŸ¥
        res = ""
        label = "error"
        print(e)
    # è¿”å› history å­—æ®µä»¥å…¼å®¹å®¢æˆ·ç«¯è§£æï¼ˆå³ä¾¿ä¸ºç©ºï¼‰
    return jsonify({"output":[res], "status":label, "history":[]})

# ==================== Webå¯è§†åŒ–ç•Œé¢ ====================
@app.route("/", methods=["GET"])
def index():
    """
    Webå¯è§†åŒ–ç•Œé¢è·¯ç”±
    
    è¿”å›ä¸€ä¸ªç¾è§‚çš„HTMLé¡µé¢ï¼Œç”¨æˆ·å¯ä»¥åœ¨æµè§ˆå™¨ä¸­ç›´æ¥ä½¿ç”¨é—®ç­”åŠŸèƒ½
    ç•Œé¢ç‰¹ç‚¹ï¼š
    - ç¾è§‚çš„æ¸å˜èƒŒæ™¯å’Œå¡ç‰‡å¼è®¾è®¡
    - è¾“å…¥æ¡†ç›´æ¥è¾“å…¥é—®é¢˜
    - æä¾›ç¤ºä¾‹é—®é¢˜ï¼Œç‚¹å‡»å³å¯å¿«é€Ÿå¡«å…¥
    - å®æ—¶æ˜¾ç¤ºç­”æ¡ˆï¼Œç•Œé¢å‹å¥½
    - æ”¯æŒCtrl+Enterå¿«æ·é”®æäº¤
    """
    html_content = """
<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>åŒ»ç–—çŸ¥è¯†å›¾è°±é—®ç­”ç³»ç»Ÿ</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        body {
            font-family: 'Microsoft YaHei', Arial, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
            padding: 20px;
        }
        .container {
            background: white;
            border-radius: 20px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
            width: 100%;
            max-width: 900px;
            padding: 40px;
        }
        h1 {
            color: #333;
            text-align: center;
            margin-bottom: 10px;
            font-size: 28px;
        }
        .subtitle {
            text-align: center;
            color: #666;
            margin-bottom: 30px;
            font-size: 14px;
        }
        .input-group {
            margin-bottom: 20px;
        }
        textarea {
            width: 100%;
            padding: 15px;
            border: 2px solid #e0e0e0;
            border-radius: 10px;
            font-size: 16px;
            resize: vertical;
            min-height: 100px;
            font-family: inherit;
        }
        textarea:focus {
            outline: none;
            border-color: #667eea;
        }
        .button-group {
            display: flex;
            gap: 10px;
            margin-bottom: 20px;
        }
        button {
            flex: 1;
            padding: 15px 30px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            border-radius: 10px;
            font-size: 16px;
            cursor: pointer;
            transition: transform 0.2s, box-shadow 0.2s;
        }
        button:hover {
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(102, 126, 234, 0.4);
        }
        button:active {
            transform: translateY(0);
        }
        button:disabled {
            background: #ccc;
            cursor: not-allowed;
            transform: none;
        }
        .answer-box {
            background: #f5f5f5;
            border-radius: 10px;
            padding: 20px;
            min-height: 150px;
            border: 2px solid #e0e0e0;
        }
        .answer-box h3 {
            color: #333;
            margin-bottom: 10px;
            font-size: 18px;
        }
        .answer-content {
            color: #555;
            line-height: 1.8;
            white-space: pre-wrap;
            word-wrap: break-word;
        }
        .loading {
            text-align: center;
            color: #667eea;
            padding: 20px;
        }
        .error {
            color: #e74c3c;
            background: #ffeaea;
            padding: 15px;
            border-radius: 5px;
            margin-top: 10px;
        }
        .example-questions {
            margin-top: 20px;
            padding-top: 20px;
            border-top: 1px solid #e0e0e0;
        }
        .example-questions h4 {
            color: #666;
            margin-bottom: 10px;
            font-size: 14px;
        }
        .example-btn {
            display: inline-block;
            padding: 8px 15px;
            margin: 5px;
            background: #f0f0f0;
            border: 1px solid #ddd;
            border-radius: 5px;
            cursor: pointer;
            font-size: 13px;
            color: #333;
            transition: all 0.2s;
        }
        .example-btn:hover {
            background: #e0e0e0;
            border-color: #667eea;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ¥ åŒ»ç–—çŸ¥è¯†å›¾è°±é—®ç­”ç³»ç»Ÿ</h1>
        <p class="subtitle">åŸºäºçŸ¥è¯†å›¾è°±æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRAGï¼‰çš„æ™ºèƒ½åŒ»ç–—é—®ç­”</p>
        
        <div class="input-group">
            <textarea id="questionInput" placeholder="è¯·è¾“å…¥æ‚¨çš„åŒ»ç–—é—®é¢˜ï¼Œä¾‹å¦‚ï¼šæˆ‘å¤´ç—›æ€ä¹ˆåŠï¼Ÿ"></textarea>
        </div>
        
        <div class="button-group">
            <button id="submitBtn" onclick="askQuestion()">æé—®</button>
            <button onclick="clearAnswer()">æ¸…ç©º</button>
        </div>
        
        <div class="answer-box" id="answerBox" style="display: none;">
            <h3>ğŸ’¡ å›ç­”ï¼š</h3>
            <div class="answer-content" id="answerContent"></div>
        </div>
        
        <div class="example-questions">
            <h4>ğŸ’¬ ç¤ºä¾‹é—®é¢˜ï¼š</h4>
            <span class="example-btn" onclick="fillQuestion('æˆ‘å¤´ç—›æ€ä¹ˆåŠ')">æˆ‘å¤´ç—›æ€ä¹ˆåŠ</span>
            <span class="example-btn" onclick="fillQuestion('é‚£å¤´ç—›æ€ä¹ˆé¢„é˜²')">é‚£å¤´ç—›æ€ä¹ˆé¢„é˜²</span>
            <span class="example-btn" onclick="fillQuestion('ä¹³è…ºç™Œçš„ç—‡çŠ¶æœ‰å“ªäº›')">ä¹³è…ºç™Œçš„ç—‡çŠ¶æœ‰å“ªäº›</span>
            <span class="example-btn" onclick="fillQuestion('å¤±çœ æ€ä¹ˆæ²»ç–—')">å¤±çœ æ€ä¹ˆæ²»ç–—</span>
            <span class="example-btn" onclick="fillQuestion('è‚ç—…è¦åƒå•¥è¯')">è‚ç—…è¦åƒå•¥è¯</span>
        </div>
    </div>

    <script>
        function fillQuestion(question) {
            document.getElementById('questionInput').value = question;
        }
        
        function clearAnswer() {
            document.getElementById('answerBox').style.display = 'none';
            document.getElementById('answerContent').innerHTML = '';
            document.getElementById('questionInput').value = '';
        }
        
        async function askQuestion() {
            const question = document.getElementById('questionInput').value.trim();
            if (!question) {
                alert('è¯·è¾“å…¥é—®é¢˜ï¼');
                return;
            }
            
            const submitBtn = document.getElementById('submitBtn');
            const answerBox = document.getElementById('answerBox');
            const answerContent = document.getElementById('answerContent');
            
            // ç¦ç”¨æŒ‰é’®ï¼Œæ˜¾ç¤ºåŠ è½½çŠ¶æ€
            submitBtn.disabled = true;
            submitBtn.textContent = 'æ€è€ƒä¸­...';
            answerBox.style.display = 'block';
            answerContent.innerHTML = '<div class="loading">ğŸ¤” æ­£åœ¨æ€è€ƒä¸­ï¼Œè¯·ç¨å€™...</div>';
            
            try {
                // ä¼˜å…ˆä½¿ç”¨RAGæ¥å£ï¼ˆå¦‚æœå¯ç”¨ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨åŸºç¡€LLMæ¥å£
                const useRAG = true; // è®¾ç½®ä¸ºtrueä½¿ç”¨RAGï¼Œfalseä½¿ç”¨åŸºç¡€LLM
                const endpoint = useRAG ? '/rag' : '/generate';
                const body = useRAG 
                    ? JSON.stringify({question: question})
                    : JSON.stringify({message: [{content: question}], max_tokens: 256});
                
                const response = await fetch(endpoint, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: body
                });
                
                const data = await response.json();
                
                if (data.status === 'success' && data.output && data.output[0]) {
                    answerContent.innerHTML = '<div class="answer-content">' + data.output[0] + '</div>';
                } else {
                    answerContent.innerHTML = '<div class="error">âŒ æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯ã€‚è¯·ç¨åé‡è¯•ã€‚</div>';
                }
            } catch (error) {
                answerContent.innerHTML = '<div class="error">âŒ ç½‘ç»œé”™è¯¯ï¼š' + error.message + '</div>';
            } finally {
                submitBtn.disabled = false;
                submitBtn.textContent = 'æé—®';
            }
        }
        
        // æ”¯æŒå›è½¦é”®æäº¤ï¼ˆCtrl+Enterï¼‰
        document.getElementById('questionInput').addEventListener('keydown', function(e) {
            if (e.key === 'Enter' && e.ctrlKey) {
                askQuestion();
            }
        });
    </script>
</body>
</html>
    """
    return html_content

# ==================== RAGå®Œæ•´é—®ç­”æ¥å£ï¼ˆå¯é€‰ï¼‰ ====================
# å¦‚æœRAGæ¨¡å—å¯ç”¨ï¼Œåˆ™åˆå§‹åŒ–RAGç›¸å…³ç»„ä»¶å¹¶æä¾›å®Œæ•´é—®ç­”æ¥å£
if RAG_AVAILABLE:
    entity_parser = QuestionClassifier()
    kg = MedicalGraph()
    rag_model = ModelAPI(MODEL_URL="http://127.0.0.1:3001/generate")
    
    # RAGé—®ç­”ç±»ï¼šæ•´åˆçŸ¥è¯†å›¾è°±æ£€ç´¢å’Œå¤§è¯­è¨€æ¨¡å‹ç”Ÿæˆ
    class KGRAG:
        """
        çŸ¥è¯†å›¾è°±æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆKGRAGï¼‰ç±»
        
        åŠŸèƒ½ï¼š
        1. å®ä½“è¯†åˆ«ï¼šä»ç”¨æˆ·é—®é¢˜ä¸­è¯†åˆ«åŒ»ç–—å®ä½“ï¼ˆç–¾ç—…ã€ç—‡çŠ¶ç­‰ï¼‰
        2. çŸ¥è¯†æ£€ç´¢ï¼šä»Neo4jçŸ¥è¯†å›¾è°±ä¸­æ£€ç´¢ç›¸å…³ä¸‰å…ƒç»„
        3. ç­”æ¡ˆç”Ÿæˆï¼šåŸºäºæ£€ç´¢åˆ°çš„çŸ¥è¯†ï¼Œä½¿ç”¨LLMç”Ÿæˆç­”æ¡ˆ
        """
        def __init__(self):
            """
            åˆå§‹åŒ–KGRAGç±»
            
            è®¾ç½®ï¼š
            - cn_dict: ä¸­æ–‡å­—æ®µåæ˜ å°„å­—å…¸ï¼ˆè‹±æ–‡->ä¸­æ–‡ï¼‰
            - entity_rel_dict: å®ä½“ç±»å‹å¯¹åº”çš„å…³ç³»å­—æ®µåˆ—è¡¨
            """
            self.cn_dict = {
                "name":"åç§°", "desc":"ç–¾ç—…ç®€ä»‹", "cause":"ç–¾ç—…ç—…å› ", "prevent":"é¢„é˜²æªæ–½",
                "cure_department":"æ²»ç–—ç§‘å®¤", "cure_lasttime":"æ²»ç–—å‘¨æœŸ", "cure_way":"æ²»ç–—æ–¹å¼",
                "cured_prob":"æ²»æ„ˆæ¦‚ç‡", "easy_get":"æ˜“æ„Ÿäººç¾¤", "belongs_to":"æ‰€å±ç§‘å®¤",
                "common_drug":"å¸¸ç”¨è¯å“", "do_eat":"å®œåƒ", "drugs_of":"ç”Ÿäº§è¯å“",
                "need_check":"è¯Šæ–­æ£€æŸ¥", "no_eat":"å¿Œåƒ", "recommand_drug":"å¥½è¯„è¯å“",
                "recommand_eat":"æ¨èé£Ÿè°±", "has_symptom":"ç—‡çŠ¶", "acompany_with":"å¹¶å‘ç—‡"
            }
            self.entity_rel_dict = {
                "disease":["prevent", "cure_way", "name", "cure_lasttime", "cured_prob", "cause", 
                          "cure_department", "desc", "easy_get", "recommand_eat", "no_eat", "do_eat", 
                          "common_drug", "drugs_of", "recommand_drug", "need_check", "has_symptom", 
                          "acompany_with", "belongs_to"],
                "symptom":["name", "has_symptom"],
            }
        
        def entity_linking(self, query):
            return entity_parser.check_medical(query)
        
        def link_entity_rel(self, query, entity, entity_type):
            cate = [self.cn_dict.get(i) for i in self.entity_rel_dict.get(entity_type, [])]
            return set(cate)
        
        def recall_facts(self, cls_rel, entity_type, entity_name, depth=1):
            entity_dict = {"disease":"Disease", "symptom":"Symptom"}
            sql = f"MATCH p=(m:{entity_dict.get(entity_type)})-[r*..{depth}]-(n) where m.name = '{entity_name}' return p"
            ress = kg.g.run(sql).data()
            direct_triples = []
            for res in ress:
                p_data = res["p"]
                nodes = p_data.nodes
                for node in nodes:
                    if node["name"] == entity_name:
                        for k, v in node.items():
                            if v != entity_name and v and self.cn_dict.get(k) in cls_rel:
                                v_str = str(v)[:200]  # æˆªæ–­é•¿æ–‡æœ¬
                                triple = f"<{node['name']},{self.cn_dict.get(k)},{v_str}>"
                                direct_triples.append(triple)
            return list(set(direct_triples))[:30]
        
        def chat(self, query):
            entity_dict = self.entity_linking(query)
            if not entity_dict:
                return "æŠ±æ­‰ï¼Œæˆ‘åœ¨çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„å®ä½“ï¼Œæ— æ³•å›ç­”ã€‚"
            facts = []
            for entity_name, types in entity_dict.items():
                for entity_type in types:
                    rels = self.link_entity_rel(query, entity_name, entity_type)
                    entity_triples = self.recall_facts(rels, entity_type, entity_name, 1)
                    facts += entity_triples
            facts = facts[:50]
            context_str = "\n".join([f"  {i+1}. {triple}" for i, triple in enumerate(facts)])
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªåŒ»ç–—çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹çŸ¥è¯†ä¸‰å…ƒç»„å›ç­”é—®é¢˜ã€‚

çŸ¥è¯†ä¸‰å…ƒç»„ï¼ˆæ ¼å¼ï¼š<å®ä½“, å…³ç³», å€¼>ï¼‰ï¼š
{context_str}

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·åŸºäºä¸Šè¿°çŸ¥è¯†ä¸‰å…ƒç»„ï¼Œç”¨ç®€æ´ã€ä¸“ä¸šçš„ä¸­æ–‡ç›´æ¥å›ç­”é—®é¢˜ã€‚å›ç­”æ—¶è¦ï¼š
1. ä¼˜å…ˆä½¿ç”¨ä¸é—®é¢˜ä¸­æåˆ°çš„å®ä½“ç›´æ¥ç›¸å…³çš„ä¸‰å…ƒç»„
2. å¦‚æœé—®é¢˜é—®"æ€ä¹ˆåŠ"æˆ–"å¦‚ä½•æ²»ç–—"ï¼Œé‡ç‚¹å…³æ³¨"æ²»ç–—æ–¹å¼"ã€"å¸¸ç”¨è¯å“"ã€"æ²»ç–—ç§‘å®¤"ç­‰å…³ç³»
3. å¦‚æœé—®é¢˜é—®"åŸå› "æˆ–"ç—…å› "ï¼Œé‡ç‚¹å…³æ³¨"ç–¾ç—…ç—…å› "å…³ç³»
4. å¦‚æœé—®é¢˜é—®"ç—‡çŠ¶"ï¼Œé‡ç‚¹å…³æ³¨"ç—‡çŠ¶"å…³ç³»
5. ä¸è¦å›ç­”ä¸çŸ¥é“æˆ–æŠ±æ­‰ï¼Œå³ä½¿ä¿¡æ¯æœ‰é™ä¹Ÿè¦ç»™å‡ºå»ºè®®

å›ç­”ï¼š"""
            answer, _ = rag_model.chat(query=prompt, history=[], max_tokens=256)
            return answer
    
    kgrag = KGRAG()
    
    @app.route("/rag", methods=["POST", "GET"])
    def rag_generate():
        """
        RAGå®Œæ•´é—®ç­”æ¥å£
        
        æ”¯æŒçš„è°ƒç”¨æ–¹å¼ï¼š
        1. POST JSONæ ¼å¼ï¼š
           {
             "question": "ç”¨æˆ·é—®é¢˜" æˆ– "q": "ç”¨æˆ·é—®é¢˜"
           }
        
        2. GET æŸ¥è¯¢å‚æ•°ï¼š
           /rag?q=ç”¨æˆ·é—®é¢˜
        
        è¿”å›æ ¼å¼ï¼š
        {
            "output": ["ç”Ÿæˆçš„ç­”æ¡ˆ"],
            "status": "success" | "error",
            "msg": "é”™è¯¯ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰"
        }
        
        æ³¨æ„ï¼š
        - æ­¤æ¥å£ä¼šå…ˆæ£€ç´¢çŸ¥è¯†å›¾è°±ï¼Œå†ç”Ÿæˆç­”æ¡ˆ
        - æ¯”åŸºç¡€/generateæ¥å£æ›´å‡†ç¡®ï¼Œä½†é€Ÿåº¦ç¨æ…¢
        """
        try:
            if request.method == "GET":
                q = request.args.get("q", "").strip()
            else:
                data = request.get_json(silent=True) or {}
                q = data.get("question", data.get("q", "")).strip()
            
            if not q:
                return jsonify({"output": [""], "status": "error", "msg": "ç¼ºå°‘é—®é¢˜"}), 400
            
            answer = kgrag.chat(q)
            return jsonify({"output": [answer], "status": "success"})
        except Exception as e:
            import traceback
            traceback.print_exc()
            return jsonify({"output": [""], "status": "error", "msg": str(e)}), 500

# ==================== å…¶ä»–è·¯ç”± ====================
@app.route("/favicon.ico", methods=["GET"])
def favicon():
    """
    å¤„ç†æµè§ˆå™¨è‡ªåŠ¨è¯·æ±‚faviconçš„æƒ…å†µï¼Œé¿å…404é”™è¯¯
    """
    return "", 204

# ==================== ä¸»ç¨‹åºå…¥å£ ====================
if __name__ == '__main__':
    """
    å¯åŠ¨FlaskæœåŠ¡
    
    é…ç½®è¯´æ˜ï¼š
    - port=3001: æœåŠ¡ç«¯å£å·
    - debug=False: ç”Ÿäº§ç¯å¢ƒå»ºè®®å…³é—­è°ƒè¯•æ¨¡å¼
    - host='0.0.0.0': å…è®¸å¤–ç½‘è®¿é—®ï¼ˆå¦‚æœéœ€è¦æœ¬åœ°è®¿é—®ï¼Œå¯æ”¹ä¸º'127.0.0.1'ï¼‰
    """
    print("[INFO] æ­£åœ¨å¯åŠ¨QwenæœåŠ¡...")
    print(f"[INFO] Webç•Œé¢åœ°å€: http://127.0.0.1:3001/")
    print(f"[INFO] APIæ¥å£åœ°å€: http://127.0.0.1:3001/generate")
    if RAG_AVAILABLE:
        print(f"[INFO] RAGæ¥å£åœ°å€: http://127.0.0.1:3001/rag")
    app.run(port=3001, debug=False, host='0.0.0.0')
